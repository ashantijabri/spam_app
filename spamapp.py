# -*- coding: utf-8 -*-
"""Spamapp.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10-XRT1EICMZrz8x7-DTpimQNiToqHqIv
"""

# Commented out IPython magic to ensure Python compatibility.
#Magic and Console Commands
# %load_ext tensorboard
!pip install keras-metrics

#System Libraries
import os
import datetime 
import sys 
from pathlib import Path
model = tf.keras.models.load_model('saved_model.pb')

#Standard Python Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pickle
import wordcloud

#Image Specific Libraries
import warnings
warnings.filterwarnings("ignore")

#Tensorflow and Keras Libraries
import tensorflow as tf
import keras
from keras import metrics
from keras.models import Sequential, Model
from keras.preprocessing.image import ImageDataGenerator
from keras.optimizers import Adam
from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization, Conv2D, MaxPooling2D
from keras import regularizers, optimizers
from tensorflow.keras import layers
from tensorflow.keras.callbacks import EarlyStopping, TensorBoard
from tensorflow.keras.preprocessing import image
from keras.applications.imagenet_utils import preprocess_input, decode_predictions
from keras_metrics import binary_average_recall, binary_f1_score, binary_false_negative, binary_false_positive, binary_precision, binary_recall, binary_true_negative, binary_true_positive
from keras.applications.resnet50 import ResNet50
from keras.applications.imagenet_utils import preprocess_input, decode_predictions
from keras.applications import vgg16
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.layers import Embedding

# helps in model building
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import Dropout
from tensorflow.keras.layers import Embedding
from tensorflow.keras.callbacks import EarlyStopping

# split data into train and test set
from sklearn.model_selection import train_test_split


#Scikit Lean Libraries
from sklearn.model_selection import train_test_split

#Google Drive Mount
from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/MyDrive/Datasets/spam.csv', encoding = 'latin-1')
df.head()

df.drop(columns=['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], inplace=True)
df

df.rename(columns = {'v1': 'label', "v2": 'text'}, inplace=True)

df.label.value_counts()

plt.figure(figsize=(12,12))

sns.countplot(df['label'])
plt.show();

df['label'] = df['label'].map({'spam': 1, "ham":0})

df

df_ham = df[df['label']==0]
df_spam = df[df['label']==1]

def show_wordcloud(df, title):
    text = ' '.join(df['text'].astype(str).tolist())
    stopwords = set(wordcloud.STOPWORDS)
    
    fig_wordcloud = wordcloud.WordCloud(stopwords=stopwords,background_color='lightgrey',
                    colormap='viridis', width=800, height=600).generate(text)
    
    plt.figure(figsize=(10,7), frameon=True)
    plt.imshow(fig_wordcloud)  
    plt.axis('off')
    plt.title(title, fontsize=20 )
    plt.show()

show_wordcloud(df_spam, "Spam messages")

show_wordcloud(df_ham, "Ham messages")

X = df['text']
y = df['label']

X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.20, random_state=808)

t = Tokenizer()
t.fit_on_texts(X_train)
encode_train = t.texts_to_sequences(X_train)
encode_test = t.texts_to_sequences(X_test)

print(encode_train[0:2])

max_length = 10
padded_train = pad_sequences(encode_train, maxlen=max_length, padding ='post')
padded_test = pad_sequences(encode_test, maxlen=max_length, padding ='post')

print(padded_train)

from tensorflow.keras.layers.experimental.preprocessing import TextVectorization

vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=200)
text_ds = tf.data.Dataset.from_tensor_slices(X_train).batch(128)
vectorizer.adapt(text_ds)

vectorizer.get_vocabulary()[:5]

voc = vectorizer.get_vocabulary()
word_index = dict(zip(voc, range(len(voc))))

!wget http://nlp.stanford.edu/data/glove.6B.zip
!unzip -q glove.6B.zip

path_to_glove_file = os.path.join(
    '/content/glove.6B.100d.txt'
)

embeddings_index = {}
with open(path_to_glove_file) as f:
    for line in f:
        word, coefs = line.split(maxsplit=1)
        coefs = np.fromstring(coefs, "f", sep=" ")
        embeddings_index[word] = coefs

print("Found %s word vectors." % len(embeddings_index))

num_tokens = len(voc) + 1
embedding_dim = 100
hits = 0
misses = 0

# Prepare embedding matrix
embedding_matrix = np.zeros((num_tokens, embedding_dim))
for word, i in word_index.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        # Words not found in embedding index will be all-zeros.
        # This includes the representation for "padding" and "OOV"
        embedding_matrix[i] = embedding_vector
        hits += 1
    else:
        misses += 1
print("Converted %d words (%d misses)" % (hits, misses))

from tensorflow.keras.layers import Embedding

embedding_layer = Embedding(
    num_tokens,
    embedding_dim,
    embeddings_initializer=keras.initializers.Constant(embedding_matrix),
    input_length=max_length,
    trainable=False,
)

vocab_size = len(t.word_index) + 1
# define the model
model = Sequential()
model.add(Embedding(vocab_size, 24, input_length=max_length))
# model.add(layers.Conv1D(128,5, padding = 'same', activation='relu'))
# model.add(layers.Conv1D(128,5, padding = 'same', activation='relu'))
# model.add(layers.AveragePooling1D(2))
# model.add(layers.BatchNormalization(axis = 1))
# model.add(layers.Conv1D(128,5, padding = 'same', activation='relu'))
# model.add(layers.Conv1D(128,5, padding = 'same', activation='relu'))
# model.add(layers.AveragePooling1D(2))
# model.add(layers.BatchNormalization(axis = 1))
model.add(Flatten())
model.add(Dense(500, activation='relu'))
model.add(Dense(200, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(100, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

# compile the model
model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])

# summarize the model
print(model.summary())

early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)

# fit the model

model.fit(x=padded_train,
         y=y_train,
         epochs=100,
         validation_data=(padded_test, y_test), verbose=1,
         callbacks=[early_stop]
         )

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

def c_report(y_true, y_pred):
   print("Classification Report")
   print(classification_report(y_true, y_pred))
   acc_sc = round(accuracy_score(y_true, y_pred), 2)
   print("Accuracy : "+ str(acc_sc))
   return acc_sc

def plot_confusion_matrix(y_true, y_pred):
   mtx = confusion_matrix(y_true, y_pred)
   sns.heatmap(mtx, annot=True, fmt='d', linewidths=.5, 
               cmap="Blues", cbar=False,xticklabels=['ham', 'spam'],
               yticklabels=['ham', 'spam'])
   plt.ylabel('True label')
   plt.xlabel('Predicted label')

preds = (model.predict(padded_test) > 0.5).astype("int32")

c_report(y_test, preds)

plot_confusion_matrix(y_test, preds)

model.save("/content/drive/MyDrive/Datasets/Models/spam_model")

with open('/content/drive/MyDrive/Datasets/Models/spam_model/tokenizer.pkl', 'wb') as output:
   pickle.dump(t, output, pickle.HIGHEST_PROTOCOL)

df = pd.DataFrame({
    
    'Text': X_test.values,
    'Actual': y_test,
    'Predictions': preds.flatten()
})
df

mis_class = df[df['Actual'] != df['Predictions']]

mis_class

s_model = tf.keras.models.load_model("/content/drive/MyDrive/Datasets/Models/spam_model")
with open('/content/drive/MyDrive/Datasets/Models/spam_model/tokenizer.pkl', 'rb') as input:
    tokenizer = pickle.load(input)

def spam_check(sms):
  sms=[sms]
  sms_proc = tokenizer.texts_to_sequences(sms)
  sms_proc = pad_sequences(sms_proc, maxlen=max_length, padding='post')
  pred = (model.predict(sms_proc) > 0.5).astype("int32").item()
  print(pred)





